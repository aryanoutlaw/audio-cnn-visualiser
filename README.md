# ğŸµ Audio CNN Visualizer

An interactive web application that visualizes how Convolutional Neural Networks classify audio using the ESC-50 dataset. Upload audio files to see real-time predictions and explore the internal feature maps of each layer.

![Audio CNN Visualizer](https://img.shields.io/badge/Status-Active-brightgreen)
![Python](https://img.shields.io/badge/Python-3.8+-blue)
![Next.js](https://img.shields.io/badge/Next.js-15-black)
![PyTorch](https://img.shields.io/badge/PyTorch-Latest-red)

## âœ¨ Features

- **ğŸ¯ Audio Classification**: Upload WAV files for real-time classification into 50 environmental sound categories
- **ğŸ§  Neural Network Visualization**: Interactive visualization of CNN feature maps and layer activations
- **ğŸ“Š Confidence Scores**: View top predictions with confidence percentages
- **ğŸ¨ Spectrogram Display**: Visualize input audio as mel-spectrograms
- **ğŸ“± Responsive UI**: Modern, clean interface built with Next.js and Tailwind CSS
- **âš¡ Real-time Processing**: Fast inference using optimized PyTorch models

## ğŸ—ï¸ Tech Stack

### Backend (ML/API)
- **PyTorch**: Deep learning framework for CNN model
- **torchaudio**: Audio processing and spectrogram generation
- **Modal**: Cloud deployment platform for ML inference
- **NumPy**: Numerical computations
- **Python 3.8+**: Core backend language

### Frontend
- **Next.js 15**: React framework with App Router
- **TypeScript**: Type-safe development
- **Tailwind CSS**: Utility-first CSS framework
- **Shadcn/ui**: Modern UI component library
- **Recharts**: Data visualization for waveforms

### Dataset & Model
- **ESC-50 Dataset**: Environmental Sound Classification with 50 classes
- **Custom CNN Architecture**: Multi-layer convolutional neural network
- **Mel-Spectrogram Input**: Audio converted to frequency domain representation

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Node.js 18+
- npm or yarn

### 1. Clone the Repository
```bash
git clone https://github.com/aryanoutlaw/audio-cnn-visualiser.git
cd audio-cnn-visualiser
```

### 2. Backend Setup
```bash
# Install Python dependencies
pip install -r requirements.txt

# Train the model (optional - pre-trained model included)
python train.py

# Test local inference
python main.py
```

### 3. Frontend Setup
```bash
cd audio-cnn-frontend

# Install dependencies
npm install

# Create environment file
echo "NEXT_PUBLIC_INFERENCE_URL=your-api-url-here" > .env.local

# Start development server
npm run dev
```

Visit `http://localhost:3000` to see the application.

## ğŸ“ Project Structure

```
audio-cnn-visualiser/
â”œâ”€â”€ README.md                   # Project documentation
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ model.py                   # CNN architecture definition
â”œâ”€â”€ train.py                   # Model training script
â”œâ”€â”€ main.py                    # Inference script
â”œâ”€â”€ best_model.pth            # Pre-trained model weights
â”œâ”€â”€ chirpingbirds.wav         # Sample audio file
â””â”€â”€ audio-cnn-frontend/       # Next.js frontend application
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ app/
    â”‚   â”‚   â”œâ”€â”€ layout.tsx     # Root layout
    â”‚   â”‚   â””â”€â”€ page.tsx       # Main application page
    â”‚   â””â”€â”€ components/ui/     # Reusable UI components
    â”‚       â”œâ”€â”€ Waveform.tsx   # Audio waveform visualization
    â”‚       â”œâ”€â”€ FeatureMap.tsx # CNN layer visualization
    â”‚       â”œâ”€â”€ ColorScale.tsx # Color legend for heatmaps
    â”‚       â””â”€â”€ ...           # Other UI components
    â”œâ”€â”€ package.json
    â””â”€â”€ next.config.js
```

## ğŸ¯ ESC-50 Sound Classes

The model can classify 50 different environmental sounds including:

**Animals**: Dog, Cat, Pig, Cow, Frog, Crickets, Chirping Birds, Hen, Insects, Sheep, Rooster, Crow

**Natural**: Rain, Sea Waves, Crackling Fire, Thunderstorm, Wind, Water Drops, Pouring Water

**Human**: Crying Baby, Sneezing, Clapping, Coughing, Footsteps, Breathing, Laughing, Brushing Teeth, Snoring, Drinking/Sipping

**Urban**: Door Wood Knock, Helicopter, Car Horn, Siren, Engine, Train, Airplane, Church Bells, Clock Alarm, Clock Tick, Fireworks

**Domestic**: Mouse Click, Keyboard Typing, Can Opening, Washing Machine, Vacuum Cleaner, Toilet Flush, Glass Breaking, Hand Saw, Chainsaw

## ğŸŒ Deployment

### Deploy Frontend to Vercel

```bash
cd audio-cnn-frontend

# Install Vercel CLI
npm i -g vercel

# Deploy
vercel

# Set environment variable in Vercel dashboard:
# NEXT_PUBLIC_INFERENCE_URL = your-modal-api-url
```

### Deploy Backend to Modal

The backend is designed to run on Modal for scalable ML inference. Update the Modal deployment configuration in your inference script.

## ğŸ”§ Model Architecture

The CNN architecture includes:
- **Input**: Mel-spectrogram (128 x 128)
- **Conv Layers**: Multiple convolutional layers with ReLU activation
- **Pooling**: Max pooling for dimensionality reduction
- **Feature Maps**: Visualizable intermediate representations
- **Output**: 50-class classification with softmax

## ğŸ“Š Performance

- **Accuracy**: ~85% on ESC-50 test set
- **Inference Time**: <200ms per audio file
- **Supported Formats**: WAV files (16kHz recommended)
- **Max Duration**: 5 seconds per clip

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

## ğŸ™ Acknowledgments

- [ESC-50 Dataset](https://github.com/karoldvl/ESC-50) for environmental sound classification
- [Modal](https://modal.com) for ML model deployment
- [Shadcn/ui](https://ui.shadcn.com) for beautiful UI components

## ğŸ“ Contact

Created by [@aryanoutlaw](https://github.com/aryanoutlaw) - feel free to reach out!

---

â­ Star this repository if you found it helpful! 